def findfreq(text, n = 3):
    """Machine learning purpose""" #extracting top freq from each sites
    en_stopws = stopwords.words('english')  #  loads the default stopwords list
    #en_stopws.append('spam')  # add custom words to the list
    text = [token for token in text if token not in en_stopws]  # filter stopwords
    frequency = nltk.FreqDist(text)
    return frequency.most_common(n)

def processeverything(text):
    """The functions which connect everything to works together"""
    processedtoken = textprocessing(text)
    processedtexts = lemmatization(processedtoken)
    topfreq = findfreq(processedtexts)
    try:
        topfreqlst.append(topfreq)
    except:
        pass
    return processedtexts

def printchange(words):
    """Testing lemmatizing modules"""
    for word in words:
        print (word + "-->"+ WordNetLemmatizer().lemmatize(word,'v'))

statistics.append(date) # 1
    statistics.append(sentimentvalue) #2
    statistics.append(greenpercent) #3
    statistics.append(redpercent) #4
    statistics.append(greenbulls) #5
    statistics.append(redbears) #6
    statistics.append(len(processedtotalword)) #7
    statistics.append(len(excludedlst)) #8
    statistics.append(len(toexclude.split())) #9
    statistics.append(greenbulls + redbears) #10